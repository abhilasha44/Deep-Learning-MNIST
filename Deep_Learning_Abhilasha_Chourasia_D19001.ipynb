{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_Abhilasha Chourasia_D19001.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skJvYAA6j0YW",
        "colab_type": "text"
      },
      "source": [
        "**Import** **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL4tW5j-eVOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "055d2b82-cfd0-43ff-bab1-5fdc8064688c"
      },
      "source": [
        "#!pip3 install torch torchvision\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "#print(\"List of files\",os.listdir(\"../input\"))\n",
        "import torch\n",
        "import numpy as np\n",
        "print(\"Torch Version:\",torch.__version__)\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch Version: 1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF_6rjqXjdIf",
        "colab_type": "text"
      },
      "source": [
        "**Helper** **Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld08bxlShoTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def test_network(net, trainloader):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # Create Variables for the inputs and targets\n",
        "    inputs = Variable(images)\n",
        "    targets = Variable(images)\n",
        "\n",
        "    # Clear the gradients from all Variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, then backward pass, then update weights\n",
        "    output = net.forward(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1qWkPJehoXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13600d67-371d-44f5-fda3-cb65e4a5b257"
      },
      "source": [
        "features=torch.randn(1,3)\n",
        "print(f'Number of Inout features:{features.shape[1]}')\n",
        "n_input=features.shape[1]   #3 input neuron\n",
        "n_hidden=2     #two hidden neuron\n",
        "n_output=1     #one output neuron\n",
        "#Weights for input to hidden layer\n",
        "W1=torch.randn(n_input,n_hidden)\n",
        "W2=torch.randn(n_hidden,n_output)\n",
        "#Bias term for hidden and output layer\n",
        "B1=torch.randn(n_hidden)\n",
        "B2=torch.randn(n_output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Inout features:3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzK_VObvhohh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using a Sigmoid Activation Function\n",
        "def activation(x):\n",
        "    return(1/1+torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJjHSTiholX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c3c34555-bb8b-485d-9d71-72d6f332dabe"
      },
      "source": [
        "h1=activation(torch.matmul(features,W1)+B1)\n",
        "print(f'Hidden Layer activations:{h1}')\n",
        "out=activation(torch.matmul(h1,W2)+B2)\n",
        "print(f'Output of the network:{out}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden Layer activations:tensor([[ 1.3230, 99.3338]])\n",
            "Output of the network:tensor([[4078836.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH5-XauOnQkL",
        "colab_type": "text"
      },
      "source": [
        "###Building our Network\n",
        "\n",
        "\n",
        "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image using MNIST data For now our goal will be to build a neural network that can take one of these images and predict the digit in the image.First, let's try to build this network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's nn module which provides a much more convenient and powerful method for defining network architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ7uHGPAhopj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary packages\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import helper\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJrYqQc6-di",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset \n",
        "First up, we need to get our dataset.Right now we will be using MNIST dataset which is already in`torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaPooFvxm6YJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "44374f0e-c908-405b-a48e-31a43c901639"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8435514.52it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 136662.30it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2318800.07it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 54296.66it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk2xP2TMm6bL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0244ebfb-527a-4ef7-80e6-25b16a081f6b"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "#Printing the size of one image\n",
        "print(images[1].numpy().squeeze().shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9SqKUCm6f8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1dfd9e54-aabe-42dd-ab85-ac24e3e4cd29"
      },
      "source": [
        "#Look at the image\n",
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/8BUqybO9JC0c8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 251,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4-XX7lFm6jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sigmoid Activation Function\n",
        "def activation(x):\n",
        "    return (1/(1+torch.exp(-x)))\n",
        "\n",
        "#Input 64x784\n",
        "inputs=images.view(images.shape[0],-1)\n",
        "#Number of input features-784\n",
        "n_input=inputs.shape[1]\n",
        "#Number of neurons in hidden layer-256\n",
        "n_hidden=256\n",
        "#Number of output neuron-10\n",
        "n_out=10\n",
        "#Weight at hidden neuron-784x256\n",
        "W1=torch.randn(n_input,n_hidden)\n",
        "#Bias at hidden neuron-256\n",
        "B1=torch.randn(n_hidden)\n",
        "#Weight at output neuron-256x10\n",
        "W2=torch.randn(n_hidden,n_out)\n",
        "#Bias at output neuron-10\n",
        "B2=torch.randn(n_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWzW-Op7m6d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "21832277-b1fe-4d1e-cb74-ccf87376248c"
      },
      "source": [
        "print(\"Shape of a batch of an image:\",images.shape)\n",
        "print(\"Shape of the input to the network:\",inputs.shape)\n",
        "print(\"Shape of the input features:\",n_input)\n",
        "print(\"Shape of the Weight matrix of neurons in the hidden layer\",W1.shape)\n",
        "print(\"Shape of the Bias vector of neurons in the hidden layer\",B1.shape)\n",
        "print(\"Shape of the Weight matrix of neurons in the output layer\",W2.shape)\n",
        "print(\"Shape of the Bias vector of neurons in the output layer\",B2.shape)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of a batch of an image: torch.Size([64, 1, 28, 28])\n",
            "Shape of the input to the network: torch.Size([64, 784])\n",
            "Shape of the input features: 784\n",
            "Shape of the Weight matrix of neurons in the hidden layer torch.Size([784, 256])\n",
            "Shape of the Bias vector of neurons in the hidden layer torch.Size([256])\n",
            "Shape of the Weight matrix of neurons in the output layer torch.Size([256, 10])\n",
            "Shape of the Bias vector of neurons in the output layer torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lso-Mv1uqZbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hidden layer activations\n",
        "h1=activation(torch.mm(inputs,W1)+B1)\n",
        "#Output layer activations\n",
        "out=activation(torch.mm(h1,W2)+B2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrG1_CaoqZeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "861c2761-5c90-4303-f8df-9afa04eacbcb"
      },
      "source": [
        "print(f'Shape of the Hidden activation of the network{h1.shape}')\n",
        "print(f'Shape of the Output of the network{out.shape}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the Hidden activation of the networktorch.Size([64, 256])\n",
            "Shape of the Output of the networktorch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45_aCO7PqZiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "21ece749-d081-47f1-a84f-82f881206805"
      },
      "source": [
        "#Let us see the network output to one of the feeded input image\n",
        "out[1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.8146e-01, 2.0596e-07, 1.8745e-01, 4.6168e-04, 4.4423e-04, 6.6391e-08,\n",
              "        9.7014e-01, 9.9996e-01, 1.0000e+00, 9.9992e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL-5KzmK7LHc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Probability Distribution using Softmax\n",
        "To calculate this probability distribution, we often use the [softmax function](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "$$\n",
        "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
        "$$\n",
        "What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waXEnkCGqZmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    return(torch.exp(x)/torch.sum(torch.exp(x),dim=1).view(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE0omfPrqzSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e135da26-488d-4340-a9e6-ddcfecee91c2"
      },
      "source": [
        "probabilities = softmax(out)\n",
        "# Does it have the right shape? Should be (64, 10)\n",
        "print(probabilities.shape)\n",
        "# Does it sum to 1?\n",
        "print(probabilities.sum(dim=1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMBC2m9P481O",
        "colab_type": "text"
      },
      "source": [
        "MULTILAYERED NETWORK\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbwEjekd25Ti",
        "colab_type": "text"
      },
      "source": [
        "We will be using ReLU() activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKS67v1qzVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets,transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBlyakY2qzZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AA18zQQUbxw",
        "colab_type": "text"
      },
      "source": [
        "Model-with 3 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5mALZumqzbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "0f3c18ce-e881-4fc1-ebdb-dfa521d9e9af"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.1 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model=Network()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))  \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model.parameters())))  "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 1.049..  Test Loss: 0.319..  Test Accuracy: 0.908\n",
            "Epoch: 2/10..  Training Loss: 0.304..  Test Loss: 0.197..  Test Accuracy: 0.942\n",
            "Epoch: 3/10..  Training Loss: 0.198..  Test Loss: 0.165..  Test Accuracy: 0.949\n",
            "Epoch: 4/10..  Training Loss: 0.150..  Test Loss: 0.108..  Test Accuracy: 0.969\n",
            "Epoch: 5/10..  Training Loss: 0.121..  Test Loss: 0.094..  Test Accuracy: 0.971\n",
            "Epoch: 6/10..  Training Loss: 0.101..  Test Loss: 0.085..  Test Accuracy: 0.975\n",
            "Epoch: 7/10..  Training Loss: 0.088..  Test Loss: 0.082..  Test Accuracy: 0.975\n",
            "Epoch: 8/10..  Training Loss: 0.076..  Test Loss: 0.079..  Test Accuracy: 0.975\n",
            "Epoch: 9/10..  Training Loss: 0.068..  Test Loss: 0.078..  Test Accuracy: 0.976\n",
            "Epoch: 10/10..  Training Loss: 0.061..  Test Loss: 0.073..  Test Accuracy: 0.979\n",
            "Total_parameters: 242762.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBM7vGumcYH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "b38f4730-69d4-4870-dd46-4acdc8fd76f0"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model=Network()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))  \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model.parameters())))  "
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 1.080..  Test Loss: 0.325..  Test Accuracy: 0.904\n",
            "Epoch: 2/10..  Training Loss: 0.340..  Test Loss: 0.201..  Test Accuracy: 0.940\n",
            "Epoch: 3/10..  Training Loss: 0.233..  Test Loss: 0.147..  Test Accuracy: 0.956\n",
            "Epoch: 4/10..  Training Loss: 0.181..  Test Loss: 0.124..  Test Accuracy: 0.963\n",
            "Epoch: 5/10..  Training Loss: 0.151..  Test Loss: 0.107..  Test Accuracy: 0.967\n",
            "Epoch: 6/10..  Training Loss: 0.129..  Test Loss: 0.100..  Test Accuracy: 0.970\n",
            "Epoch: 7/10..  Training Loss: 0.114..  Test Loss: 0.086..  Test Accuracy: 0.973\n",
            "Epoch: 8/10..  Training Loss: 0.101..  Test Loss: 0.079..  Test Accuracy: 0.976\n",
            "Epoch: 9/10..  Training Loss: 0.092..  Test Loss: 0.077..  Test Accuracy: 0.977\n",
            "Epoch: 10/10..  Training Loss: 0.083..  Test Loss: 0.085..  Test Accuracy: 0.974\n",
            "Total_parameters: 242762.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMd-7LAqTizy",
        "colab_type": "text"
      },
      "source": [
        "Model_2 with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qykTuCyDSdEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "6f807ddc-42e3-44e4-8f7a-8922488ffb9e"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        #self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.1 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_2=Network()\n",
        "optimizer=optim.SGD(model_2.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_2(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_2.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_2(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_2.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset))) \n",
        "        #print(\"Our model: \\n\\n\", model_2, '\\n'),\n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_2.parameters())))\n",
        "        #pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "        #pytorch_total_params   "
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.709..  Test Loss: 0.315..  Test Accuracy: 0.905\n",
            "Epoch: 2/10..  Training Loss: 0.285..  Test Loss: 0.209..  Test Accuracy: 0.937\n",
            "Epoch: 3/10..  Training Loss: 0.208..  Test Loss: 0.154..  Test Accuracy: 0.955\n",
            "Epoch: 4/10..  Training Loss: 0.163..  Test Loss: 0.129..  Test Accuracy: 0.961\n",
            "Epoch: 5/10..  Training Loss: 0.134..  Test Loss: 0.126..  Test Accuracy: 0.960\n",
            "Epoch: 6/10..  Training Loss: 0.115..  Test Loss: 0.092..  Test Accuracy: 0.971\n",
            "Epoch: 7/10..  Training Loss: 0.100..  Test Loss: 0.086..  Test Accuracy: 0.974\n",
            "Epoch: 8/10..  Training Loss: 0.088..  Test Loss: 0.080..  Test Accuracy: 0.977\n",
            "Epoch: 9/10..  Training Loss: 0.079..  Test Loss: 0.071..  Test Accuracy: 0.977\n",
            "Epoch: 10/10..  Training Loss: 0.071..  Test Loss: 0.071..  Test Accuracy: 0.977\n",
            "Total_parameters: 235146.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z81_wHlUc3Fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "a3202192-b46f-47be-a87d-79aeb0b6d0d0"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        #self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_2=Network()\n",
        "optimizer=optim.SGD(model_2.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_2(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_2.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_2(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_2.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset))) \n",
        "        #print(\"Our model: \\n\\n\", model_2, '\\n'),\n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_2.parameters())))\n",
        "        #pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "        #pytorch_total_params   "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.743..  Test Loss: 0.290..  Test Accuracy: 0.917\n",
            "Epoch: 2/10..  Training Loss: 0.297..  Test Loss: 0.210..  Test Accuracy: 0.939\n",
            "Epoch: 3/10..  Training Loss: 0.217..  Test Loss: 0.156..  Test Accuracy: 0.954\n",
            "Epoch: 4/10..  Training Loss: 0.174..  Test Loss: 0.130..  Test Accuracy: 0.961\n",
            "Epoch: 5/10..  Training Loss: 0.148..  Test Loss: 0.111..  Test Accuracy: 0.965\n",
            "Epoch: 6/10..  Training Loss: 0.126..  Test Loss: 0.097..  Test Accuracy: 0.970\n",
            "Epoch: 7/10..  Training Loss: 0.113..  Test Loss: 0.092..  Test Accuracy: 0.971\n",
            "Epoch: 8/10..  Training Loss: 0.101..  Test Loss: 0.084..  Test Accuracy: 0.973\n",
            "Epoch: 9/10..  Training Loss: 0.091..  Test Loss: 0.079..  Test Accuracy: 0.974\n",
            "Epoch: 10/10..  Training Loss: 0.084..  Test Loss: 0.077..  Test Accuracy: 0.977\n",
            "Total_parameters: 235146.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJGhBko8TYW1",
        "colab_type": "text"
      },
      "source": [
        "Model_1 with 1 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBZtSPfLSdH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "9d08f64b-ad43-42ea-ad1b-7e0298a72b1c"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        #self.fc3 = nn.Linear(128, 10)\n",
        "        #self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.1 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        #x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1=Network()\n",
        "optimizer=optim.SGD(model_1.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_1(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_1.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_1(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_1.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))   \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_1.parameters()))) "
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.577..  Test Loss: 0.307..  Test Accuracy: 0.912\n",
            "Epoch: 2/10..  Training Loss: 0.295..  Test Loss: 0.240..  Test Accuracy: 0.933\n",
            "Epoch: 3/10..  Training Loss: 0.237..  Test Loss: 0.202..  Test Accuracy: 0.942\n",
            "Epoch: 4/10..  Training Loss: 0.199..  Test Loss: 0.174..  Test Accuracy: 0.951\n",
            "Epoch: 5/10..  Training Loss: 0.172..  Test Loss: 0.148..  Test Accuracy: 0.957\n",
            "Epoch: 6/10..  Training Loss: 0.151..  Test Loss: 0.134..  Test Accuracy: 0.962\n",
            "Epoch: 7/10..  Training Loss: 0.135..  Test Loss: 0.125..  Test Accuracy: 0.964\n",
            "Epoch: 8/10..  Training Loss: 0.123..  Test Loss: 0.113..  Test Accuracy: 0.968\n",
            "Epoch: 9/10..  Training Loss: 0.112..  Test Loss: 0.108..  Test Accuracy: 0.968\n",
            "Epoch: 10/10..  Training Loss: 0.103..  Test Loss: 0.101..  Test Accuracy: 0.971\n",
            "Total_parameters: 203530.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqucJyy9khSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "02754a97-5215-4d65-957e-41a2bb609be7"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        #self.fc3 = nn.Linear(128, 10)\n",
        "        #self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        #x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1=Network()\n",
        "optimizer=optim.SGD(model_1.parameters(),lr=0.05)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_1(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_1.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_1(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_1.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))   \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_1.parameters()))) "
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.596..  Test Loss: 0.301..  Test Accuracy: 0.917\n",
            "Epoch: 2/10..  Training Loss: 0.300..  Test Loss: 0.240..  Test Accuracy: 0.931\n",
            "Epoch: 3/10..  Training Loss: 0.240..  Test Loss: 0.197..  Test Accuracy: 0.944\n",
            "Epoch: 4/10..  Training Loss: 0.202..  Test Loss: 0.167..  Test Accuracy: 0.952\n",
            "Epoch: 5/10..  Training Loss: 0.175..  Test Loss: 0.149..  Test Accuracy: 0.957\n",
            "Epoch: 6/10..  Training Loss: 0.155..  Test Loss: 0.132..  Test Accuracy: 0.962\n",
            "Epoch: 7/10..  Training Loss: 0.141..  Test Loss: 0.122..  Test Accuracy: 0.964\n",
            "Epoch: 8/10..  Training Loss: 0.129..  Test Loss: 0.111..  Test Accuracy: 0.967\n",
            "Epoch: 9/10..  Training Loss: 0.118..  Test Loss: 0.107..  Test Accuracy: 0.967\n",
            "Epoch: 10/10..  Training Loss: 0.109..  Test Loss: 0.100..  Test Accuracy: 0.971\n",
            "Total_parameters: 203530.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tg5sNWUvrQu",
        "colab_type": "text"
      },
      "source": [
        "New Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK8BiTDwvpyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "68eff5c6-a92c-4174-f7ec-9e66d2f7e74d"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 150)\n",
        "        self.fc3 = nn.Linear(150, 75)\n",
        "        self.fc4 = nn.Linear(75, 10)\n",
        "\n",
        "        # Dropout module with 0.1 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_3=Network()\n",
        "optimizer=optim.SGD(model_3.parameters(),lr=0.1)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_3(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_3.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_3(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_3.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))   \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_3.parameters()))) "
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.679..  Test Loss: 0.225..  Test Accuracy: 0.929\n",
            "Epoch: 2/10..  Training Loss: 0.186..  Test Loss: 0.126..  Test Accuracy: 0.961\n",
            "Epoch: 3/10..  Training Loss: 0.128..  Test Loss: 0.093..  Test Accuracy: 0.971\n",
            "Epoch: 4/10..  Training Loss: 0.100..  Test Loss: 0.086..  Test Accuracy: 0.973\n",
            "Epoch: 5/10..  Training Loss: 0.083..  Test Loss: 0.074..  Test Accuracy: 0.977\n",
            "Epoch: 6/10..  Training Loss: 0.068..  Test Loss: 0.070..  Test Accuracy: 0.979\n",
            "Epoch: 7/10..  Training Loss: 0.058..  Test Loss: 0.068..  Test Accuracy: 0.979\n",
            "Epoch: 8/10..  Training Loss: 0.050..  Test Loss: 0.063..  Test Accuracy: 0.981\n",
            "Epoch: 9/10..  Training Loss: 0.045..  Test Loss: 0.068..  Test Accuracy: 0.981\n",
            "Epoch: 10/10..  Training Loss: 0.040..  Test Loss: 0.079..  Test Accuracy: 0.976\n",
            "Total_parameters: 292735.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEq1bjpU4VyS",
        "colab_type": "text"
      },
      "source": [
        "model_4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnJTV7Pj4UM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "23869dbd-97ca-4eb9-d9f1-245e0638f739"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True,num_workers=0)\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 150)\n",
        "        self.fc3 = nn.Linear(150, 10)\n",
        "        #self.fc4 = nn.Linear(75, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_4=Network()\n",
        "optimizer=optim.SGD(model_4.parameters(),lr=0.6)\n",
        "criterion=nn.NLLLoss()\n",
        "epochs=10\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        #images=images.view(images.shape[0],-1)\n",
        "        log_ps=model_4(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138 \n",
        "        \n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model_4.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model_4(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model_4.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))   \n",
        "print(\"Total_parameters: {:.3f}.. \".format(sum(p.numel() for p in model_4.parameters()))) "
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.354..  Test Loss: 0.151..  Test Accuracy: 0.952\n",
            "Epoch: 2/10..  Training Loss: 0.143..  Test Loss: 0.109..  Test Accuracy: 0.967\n",
            "Epoch: 3/10..  Training Loss: 0.109..  Test Loss: 0.088..  Test Accuracy: 0.973\n",
            "Epoch: 4/10..  Training Loss: 0.092..  Test Loss: 0.093..  Test Accuracy: 0.971\n",
            "Epoch: 5/10..  Training Loss: 0.079..  Test Loss: 0.116..  Test Accuracy: 0.965\n",
            "Epoch: 6/10..  Training Loss: 0.071..  Test Loss: 0.072..  Test Accuracy: 0.977\n",
            "Epoch: 7/10..  Training Loss: 0.063..  Test Loss: 0.083..  Test Accuracy: 0.975\n",
            "Epoch: 8/10..  Training Loss: 0.057..  Test Loss: 0.092..  Test Accuracy: 0.973\n",
            "Epoch: 9/10..  Training Loss: 0.053..  Test Loss: 0.066..  Test Accuracy: 0.981\n",
            "Epoch: 10/10..  Training Loss: 0.047..  Test Loss: 0.100..  Test Accuracy: 0.974\n",
            "Total_parameters: 282160.000.. \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}